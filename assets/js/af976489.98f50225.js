"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1885],{3133:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>a,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>h});var n=t(4848),s=t(8453);const r={},o="Redshift",l={id:"rendering/redshift",title:"Redshift",description:"- Random color per face.",source:"@site/docs/rendering/redshift.md",sourceDirName:"rendering",slug:"/rendering/redshift",permalink:"/docs/rendering/redshift",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/rendering/redshift.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Octane",permalink:"/docs/rendering/octane"},next:{title:"Vray",permalink:"/docs/rendering/vray (LAPTOP-ECKQA8BC's conflicted copy 2024-09-20)"}},a={},h=[{value:"General",id:"general",level:2},{value:"<em>Preparing first few levels of ray tracing hierarchy takes so long</em>",id:"preparing-first-few-levels-of-ray-tracing-hierarchy-takes-so-long",level:2},{value:"Hair",id:"hair",level:2},{value:"Compositing:",id:"compositing",level:2},{value:"Nodes",id:"nodes",level:2},{value:"Render Settings",id:"render-settings",level:2},{value:"Global Illumination",id:"global-illumination",level:3},{value:"Sampling",id:"sampling",level:3},{value:"Camera",id:"camera",level:2},{value:"Volume",id:"volume",level:2},{value:"Cache",id:"cache",level:3},{value:"Deprecated Infos",id:"deprecated-infos",level:2},{value:"Lighting",id:"lighting",level:2},{value:"Transmissive Materials",id:"transmissive-materials",level:2},{value:"Subsurface",id:"subsurface",level:2},{value:"UDIM",id:"udim",level:2},{value:"Proxy",id:"proxy",level:2},{value:"Portals",id:"portals",level:2},{value:"Misc",id:"misc",level:2},{value:"Motion Blur",id:"motion-blur",level:2},{value:"False Colour",id:"false-colour",level:2},{value:"Diagnostics",id:"diagnostics",level:2},{value:"Interiors",id:"interiors",level:2}];function d(e){const i={a:"a",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.h1,{id:"redshift",children:"Redshift"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Random color per face."}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"CANON RESPONSE FOR HDRI and PORTALS?"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"general",children:"General"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Ok, but a lower rez bitmap (or lower mip stack level) will produce a different specular and bump due to filtering thus giving you a false positive result."}),"\n",(0,n.jsx)(i.li,{children:"colorful foggy class = use an actual colored object but just hide it in the main ray and show it in the refraction ray\nA small detail can only be judged properly at the final rez with the full texture, you might even need to use a negative bias on your texture to extract a higher rez map from the mip stack. Doing the exact opposite might not be the best idea."}),"\n",(0,n.jsx)(i.li,{children:"Height and displacement are considered the same thing, where height is usually used on large scale (ie terrain) and displacement on smaller scale (ie hero asset/prop). Both of these will displace and change the geometry."}),"\n",(0,n.jsx)(i.li,{children:"sheen tint is nice if the base color of the cloth is black."}),"\n",(0,n.jsx)(i.li,{children:"Parrallax Bump OLS Node to Simulate the 2D Texture for Redshift"}),"\n",(0,n.jsx)(i.li,{children:"The Maxon noise is a 3d texture so any projection will be useless. By default it's stuck in object space."}),"\n",(0,n.jsx)(i.li,{children:"If you really want to turn it into a 2d noise, then you'll have to switch to a UV Vertex mode istead of object/world, that will honor the projection mode in the material tag but you will lose the freedom of a 3d texture"}),"\n",(0,n.jsx)(i.li,{children:"randomize pattern every frame turned off when using denoiser"}),"\n",(0,n.jsx)(i.li,{children:"better not to use the auto bumping. just use the height map fir the bump node. that way you can control the details much better."}),"\n",(0,n.jsx)(i.li,{children:"If the scene contains a lot of geometric detail (example: foliage covering a big part of the screen), too many irradiance cache points might have to be generated."}),"\n",(0,n.jsx)(i.li,{children:"super bright light affects the rendering times."}),"\n",(0,n.jsx)(i.li,{children:"gaseous noise for the puddle"}),"\n",(0,n.jsx)(i.li,{children:"AO texture is towards the overall tint. or color layer in multiply."}),"\n",(0,n.jsx)(i.li,{children:"use the gaseous noise for the puddles"}),"\n",(0,n.jsx)(i.li,{children:"using maxon noise to break some up generic roughness. using high and low clip"}),"\n",(0,n.jsx)(i.li,{children:"offsetion texture node. its lifts the black."}),"\n",(0,n.jsx)(i.li,{children:"Use Displacement Node. But need also to have a redshift tag on the object,. Set the maximum duisplacement and displacement scale"}),"\n",(0,n.jsx)(i.li,{children:"AO texture (Out Color) > In Color Multiplier (Albedo or Diffuse)"}),"\n",(0,n.jsx)(i.li,{children:"Gama ovverride on noncolor data"}),"\n",(0,n.jsx)(i.li,{children:"Using the Ramp as a mediator."}),"\n",(0,n.jsx)(i.li,{children:"When denoising is used with an animation we recommend disabling Random Noise Pattern!"}),"\n",(0,n.jsx)(i.li,{children:"Choosing a filter type and filter size highly depends on what you are rendering. For example, still images can often get away with sharpening filters such as Mitchell or Lanczos. Animations, on the other hand, usually work best with blurrier filters, like Gauss. The reason is that 'jaggies' are particularly visible on animations!"}),"\n",(0,n.jsx)(i.li,{}),"\n",(0,n.jsx)(i.li,{children:"1.8 on metals"}),"\n",(0,n.jsx)(i.li,{children:"metalness is usually brdf"}),"\n",(0,n.jsx)(i.li,{children:"this kind of shot is a path/ray tracing engine's worst nightmare. the numerous small light sources + harsh light from the lightbox (the fireflies you are describing are caused by too high intensity light"}),"\n",(0,n.jsx)(i.li,{children:"Before we render let's take a look at at the Optimization tab in Redshift's Render Settings. We are going to enable Hair Min Pixel Width which will allow for cleaner results without having to crank up our unified sampling."}),"\n",(0,n.jsx)(i.li,{children:"Stacking C4D Native Hair Material for geoemetric changes is supported in redshift."}),"\n",(0,n.jsx)(i.li,{children:"Unable to use UDIM on Hairs in C4D. You need to have separate hair object for every tile"}),"\n",(0,n.jsx)(i.li,{children:'"Auto" in RS will pick "srgb" for things like png, jpeg, and it will pick "raw" (linear) for stuff like tiff, exr'}),"\n",(0,n.jsx)(i.li,{children:"redshift scatter is in the settings scatter tag not in the native matrix/scatter mograph object"}),"\n",(0,n.jsx)(i.li,{}),"\n",(0,n.jsx)(i.li,{children:"the main reason for crashes is using too much GPU VRAM (for hair and/or textures), since RT is not out-of-core."}),"\n",(0,n.jsx)(i.li,{children:"rs matrix is much faster than rs cloner."}),"\n",(0,n.jsx)(i.li,{children:"Ah indeed Freeze tesselation does seem to speed up things a lot, thanks for that tip."}),"\n",(0,n.jsxs)(i.li,{children:["By default, the Redshift plugin will extract and process all the mesh objects geometry and materials for each frame while rendering a sequence.","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"If you are rendering an animation without mesh changes (for example an animation with only camera, lights or objects transformations), you can disable the \u201cSequence Rendering Full Scene Reload\u201d option in the RS ROP node to eliminate near all the extraction time among the frames."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{children:"C:\\ProgramData\\Redshift\\Log\\"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"preparing-first-few-levels-of-ray-tracing-hierarchy-takes-so-long",children:(0,n.jsx)(i.em,{children:"Preparing first few levels of ray tracing hierarchy takes so long"})}),"\n",(0,n.jsx)(i.h2,{id:"hair",children:"Hair"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:'"Render Polygons as Hair" option speeds up the render'}),"\n",(0,n.jsx)(i.li,{}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"compositing",children:"Compositing:"}),"\n",(0,n.jsx)(i.p,{children:"Allow Overexpesure to 1 in Redshift\nInvert ACES SDR Video to have a more faithful color"}),"\n",(0,n.jsx)(i.p,{children:"Managing Color - Part 2 (of 5)\nNice After Effects ACES Set Up\nUnchecking Color View Space if you want to color grade it more."}),"\n",(0,n.jsx)(i.p,{children:"color correct befre OCIO\nalways activate SIM Card lock sa phone"}),"\n",(0,n.jsx)(i.p,{children:"Managing Color - Part 3 (of 5)\nLighting Groups\nLumetri Scope to compute the depth range"}),"\n",(0,n.jsx)(i.h2,{id:"nodes",children:"Nodes"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Uber Shader","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"So many interation. Used only for legacy. Not being developed. The latest one is the optimized."}),"\n",(0,n.jsx)(i.li,{children:"Architectural"}),"\n",(0,n.jsx)(i.li,{children:"Material"}),"\n",(0,n.jsx)(i.li,{children:"Standard Material."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{children:"s"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"render-settings",children:"Render Settings"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Let\u2019s also enable AOV \xa0clamping to ensure that our pixel values stay reasonable."}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"global-illumination",children:"Global Illumination"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:'Thankfully Redshift contains a "force-brute-force" per-object option so that such problematic objects can use brute-force instead, while everything else uses the irradiance cache'}),"\n",(0,n.jsx)(i.li,{children:"Irradiance cache, irradiance point cloud. These are not needed because progressive rendering computes GI in a brute-force way."}),"\n",(0,n.jsx)(i.li,{children:"Photon Mapping is less efficient, but can be used for Reflective and Refractive caustics"}),"\n",(0,n.jsx)(i.li,{children:"Lower screen radius, increase performance on the irradiance."}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"sampling",children:"Sampling"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Sampling in RS works by ",(0,n.jsx)(i.strong,{children:"dividing the localized samples"})," (lights, reflection, refraction, GI, AO, Volume, SSS etc etc) by the maxUnified then the value is multiplied by the minUnified, then if the result is noisy (according to the adaptive error threshold) it shoots more samples until it considers there's no more noise or it reaches the maxUnified or localized samples value which one is bigger."]}),"\n",(0,n.jsx)(i.li,{}),"\n",(0,n.jsxs)(i.li,{children:["For the most part, the only thing you need to worry about it. When Show Samples is enabled, the noisy parts of the image will ",(0,n.jsx)(i.strong,{children:"typically be brighter because the unified sampler"})," will shoot more rays to try and clean up the noise."]}),"\n",(0,n.jsx)(i.li,{children:"Its way more xpensive send camera rays than it is reflection rays"}),"\n",(0,n.jsx)(i.li,{children:"Sample light 200-500. Smooth na"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"camera",children:"Camera"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Maya Depth of Field is supported if there are no bokeh applied"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"volume",children:"Volume"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Official Docs: ",(0,n.jsx)(i.a,{href:"https://help.maxon.net/r3d/houdini/en-us/#html/Volume+Object.html?TocPath=Volume%2520Topics%257C_____1",children:"https://help.maxon.net/r3d/houdini/en-us/#html/Volume+Object.html?TocPath=Volume%2520Topics%257C_____1"})]}),"\n",(0,n.jsx)(i.li,{children:"RS volume needs to be in a null for it to work in a cloner"}),"\n",(0,n.jsxs)(i.li,{children:["For example, temperature should be used to drive volume emission for realistic flames and explosions by making use of ",(0,n.jsx)(i.a,{href:"https://help.maxon.net/r3d/houdini/en-us/Content/html/Volume+Shader.html#Emission2",children:"blackbody emission"})," rather than using the density channel which is tailored for describing thick plumes of smoke or clouds."]}),"\n",(0,n.jsx)(i.li,{children:"Velocity Channels for Motion Blur"}),"\n",(0,n.jsx)(i.li,{children:"For C4D, there is a dedicated RS Volume object. For Houdini, there isn't. It is a setting in the Redshift OBJ tab in Geo Object Context."}),"\n",(0,n.jsx)(i.li,{children:"Volume grids may also be referred to as channels, particularly in a shading context, and Redshift uses the channel terminology when referencing a VDB\xa0grid in a volume shader."}),"\n",(0,n.jsxs)(i.li,{children:["The volume shader is divided into three shading components: ",(0,n.jsx)(i.em,{children:"Scatter"}),", ",(0,n.jsx)(i.em,{children:"Absorption"})," and ",(0,n.jsx)(i.em,{children:"Emission"}),".","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.em,{children:"Scatter"}),' as similar to "diffuse" shading,']}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.em,{children:"Absorption"}),' like volume "transparency," and']}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.em,{children:"Emission"}),' as "incandescence" or "self-illumination."']}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(i.li,{children:["Therefore, if you want to make your volume brighter or darker, you should adjust the ",(0,n.jsx)(i.em,{children:"Scatter"})," parameters. If you want to make your volume appear more solid or dense, you should adjust the ",(0,n.jsx)(i.em,{children:"Absorption"})," parameters. Finally, if your volume represents a self-illuminating effect like fire and want to make it brighter or dimmer, you should adjust the ",(0,n.jsx)(i.em,{children:"Emission"})," parameters."]}),"\n",(0,n.jsxs)(i.li,{children:["Prerequisite:","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"At least one channel must be set from the volume object to drive a volume's scattering or emission."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{children:"At least one light must be present in the scene with volume contribution set to a value greater than 0 (by default lights have a volume contribution of 1)."}),"\n",(0,n.jsx)(i.li,{children:"The RS\xa0Volume node must be connected to the 'Volume' input of an Output node."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(i.li,{children:["For the most realistic results higher volume trace depths will be required, but if faster render times are more important try adjusting the ",(0,n.jsx)(i.a,{href:"https://help.maxon.net/r3d/houdini/en-us/Content/html/Volume+Shader.html?tocpath=Volume%20Topics%7C_____2#Scatteri",children:"scatter / absorption coefficient ratio"})," and decreasing the ",(0,n.jsx)(i.a,{href:"https://help.maxon.net/r3d/houdini/en-us/Content/html/Volume+Shader.html?tocpath=Volume%20Topics%7C_____2#Shadow",children:"shadow density scale"})," first and then increase the volume trace depth slowly as needed."]}),"\n"]}),"\n",(0,n.jsx)(i.h3,{id:"cache",children:"Cache"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Redshift creates a cache for every texture. Like in Arnold, it helps speed up loading time during render time. I was able to render 40-50GB of textures with no problem, unlike with Blender/Cycles where it just freezes and crashes. Unlike Arnold, this caching feature can't be disabled."}),"\n",(0,n.jsx)(i.li,{children:"What's the cache, i mean catch? It stores this cache drive, almost indefinitely, and consequently eats a lot of drive."}),"\n",(0,n.jsxs)(i.li,{children:["If you are running out of memory, you can manually delete them at ",(0,n.jsx)(i.code,{children:"C:\\Users\\BT\\AppData\\Local\\Redshift\\Cache"})," or perform the deleting cache inside the Redshift Menu of the DCC"]}),"\n",(0,n.jsx)(i.li,{children:"You can also set the cache directory with the project directory. This is the Arnold default."}),"\n",(0,n.jsx)(i.li,{children:"You can also pre process those cache tools before hand. Copy Pre-Converted Textures to Cache Folder"}),"\n",(0,n.jsx)(i.li,{children:"Texture Processor Tool"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"deprecated-infos",children:"Deprecated Infos"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Renderview is newer than IPR"}),"\n",(0,n.jsx)(i.li,{children:"In redshift, normal map is being phase out. Use bump map instead."}),"\n",(0,n.jsxs)(i.li,{children:["Backlighting in Material.","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"So to get the old style \u201cbacklighting\u201d you turn on thin walled and use the controls found in the subsurface section. It will gray out the controls that don\u2019t work / influence the backlighting for you. Since backlighting is technically a form of SSS just not a full blown multiscatter like it is when you disable thin walled."}),"\n",(0,n.jsx)(i.li,{children:"just activate thin walled, the SSS actually turns into true backlighting once you turn on thin walled. As it is energy conserving unlike the old one"}),"\n",(0,n.jsx)(i.li,{children:"It is now done using \"Subsurface\" and by setting it to thin-walled. Do know that with the new model, raising subsurface darkens diffuse, so it's a balance. This is technically correct since it's energy conserving (the shader doesn't produce more energy than it receives)."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"lighting",children:"Lighting"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"You got hard edges because of the sun light which is hard, you need to increase the shadow softness of the sun to get soft shadows (by a lot, try 10)."}),"\n",(0,n.jsx)(i.li,{children:"redshift dome light should be have the sky color space of scene-linear-rec-709-srgb"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"transmissive-materials",children:"Transmissive Materials"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"make metalness set to 1. and have the base color set to black. to make the thin film more visible. for debugging."}),"\n",(0,n.jsx)(i.li,{children:"if you want to appply to bubbles, just set transmission to 1."}),"\n",(0,n.jsx)(i.li,{}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"subsurface",children:"Subsurface"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"The subsurface scatter colour should be our character\u2019s skin pigment. It determines the colour shift as light enters the skin. The Scatter colour \xa0should be closer to blood red than the character\u2019s skin itself. After all, this is what determines how the light\u2019s colour shifts as it travels inside the skin."}),"\n",(0,n.jsx)(i.li,{children:"skin. there's not much subsurface scattering. like 0.1 or 0.2 lang jud. dili full"}),"\n",(0,n.jsx)(i.li,{children:"Skin IOR 1.44"}),"\n",(0,n.jsx)(i.li,{children:"Our Scatter Radius is just how far the light will be able to travel inside the skin, and the Scale value can multiply this. The Diffuse Amount weakens"}),"\n",(0,n.jsx)(i.li,{children:"the subsurface scattering effect. SSS Reflections attribute work similarly to RSA Reflections."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"udim",children:"UDIM"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Redshift doesn't recognize UDIM if there is no 1001 map present. Workaround is to make a dummy 1001 map."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"proxy",children:"Proxy"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"creating proxy in redshift? fudge. its not on the redshift menu dropdown.it's on the native File > export (rs proxy dropdown)"}),"\n",(0,n.jsx)(i.li,{children:"When placing multiple copies of a Redshift Proxy in a scene, it is much more efficient for memory and performance to create a single Redshift Proxy and then create multiple instances of this proxy and place them as desired."}),"\n",(0,n.jsx)(i.li,{children:"Animated proxies can be heavyweight because they are per-frame caches."}),"\n",(0,n.jsx)(i.li,{children:"You cannot retrieve the original Maya, 3ds Max or Softimage mesh from a Redshift Proxy File, so it is advisable to keep the original source data that was used to export the Redshift Proxy"}),"\n",(0,n.jsx)(i.li,{children:"The data exported with your Redshift Proxy is determined by your Render Settings at the time of export, by default Redshift will try to discard as much unnecessary data as possible to achieve the most efficient file size."}),"\n",(0,n.jsx)(i.li,{children:"When exporting a set of objects as a Redshift Proxy File, only the vertex attributes used by the currently assigned shaders are included in the exported data."}),"\n",(0,n.jsx)(i.li,{children:"In order for Redshift Proxies to render with Motion Blur you must make sure that motion blur is enabled in your Render Settings at the time of export."}),"\n",(0,n.jsx)(i.li,{children:"Adaptive tessellation cannot be used when instancing proxies in Redshift."}),"\n",(0,n.jsx)(i.li,{children:"need to setup motion blur before exporting proxy files."}),"\n",(0,n.jsx)(i.li,{children:"Two UVs for RS proxy. not working."}),"\n",(0,n.jsxs)(i.li,{children:["Houdini Proxies","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"two methods in exporting RS proxy in Houdini. SOP or ROP level."}),"\n",(0,n.jsx)(i.li,{children:"Just use the switch node to switch along frames."}),"\n",(0,n.jsx)(i.li,{children:"Better to use ROP node and just tick the Archive Tab. Mas stable daw."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"portals",children:"Portals"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Place portal lights that are slightly larger than the windows right outside."}),"\n",(0,n.jsx)(i.li,{children:"The Portal light blocks and absorbs Global Illumination rays. It works by sampling light color from the RS Environment shader, either from the Default Environment (see Redshift Render Preferences/Globals/Options), from a Redshift Camera Environment or the Custom Environment link of the Portal itself (by asigning RS Environment Materials)."}),"\n",(0,n.jsx)(i.li,{children:"In your scene, make sure that the Z-axis of the portal is directed towards the interior, as the ambient light is emitted only along the positive Z-axis of this light."}),"\n",(0,n.jsxs)(i.li,{children:["Light Sampling Parameters","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"The following parameters are only visible and relevant when Automatic Sampling is disabled."}),"\n",(0,n.jsx)(i.li,{children:"Generally, the larger and more intense a light is, the more samples it needs to produce noise-free results."}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.li,{children:"Also you \"can\" use portals with the HDR Dome Light, but it will block the GI from objects in the exterior, but if you don't have anything outside which is bright enough to cast GI inside that it should be lighting the interior somewhat then it shouldn't matter."}),"\n",(0,n.jsx)(i.li,{children:"This is a limitation of how mip mapping works on envs compared to the dome light which is using Multi Importance Sampling (different techniques with slightly different results).\nYou'll probably get the same if instead of a dome light you'll use an env shader.\nWe don't have a solution for now. You just have to account for the differences."}),"\n",(0,n.jsx)(i.li,{children:"A portal light uses the Env shader not the Env object so create a rsEnv shader,"}),"\n",(0,n.jsx)(i.li,{children:"No, light portals don't work togather with Domelight, you either lit your interior with Domelight or portals+environment map."}),"\n",(0,n.jsx)(i.li,{}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"misc",children:"Misc"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"C:\\ProgramData\\Redshift\\Log"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"motion-blur",children:"Motion Blur"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"motion blur in redshift is in render settings not in the camera settinggs"}),"\n",(0,n.jsx)(i.li,{children:"motion blur worksi n full render not in IPR"}),"\n",(0,n.jsx)(i.li,{children:"there is a camera blur where object is not moving but the camera is moving"}),"\n",(0,n.jsx)(i.li,{children:"There is a deformation blur where only points are moving"}),"\n",(0,n.jsx)(i.li,{children:"In case anyone else runs into this issue... adding a Motion Vectors AOV DISABLES motion blur. Delete the AOV and you're good."}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"false-colour",children:"False Colour"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"There are two ways of knowing, one is by using the Histogram in the Color Controls and another is by using a specialized LUT."}),"\n",(0,n.jsx)(i.li,{children:"Redshift actually ships with one under the Filmic LUTs section called \u201cFalse Colour\u201d"}),"\n",(0,n.jsx)(i.li,{children:"Its located in the Data folder where your Redshift Install is. For example  C:\\ProgramData\\Redshift\\Data\\LUT\\FilmicLUTs"}),"\n",(0,n.jsx)(i.li,{children:"False Colour. This LUT is an extremely useful tool for evaluating your image in terms of the dynamic range and latitude. It is a color coded \u201cheat map\u201d of your image values, according to the following codes:"}),"\n"]}),"\n",(0,n.jsxs)(i.p,{children:["Value Colour Scene Referred Value",(0,n.jsx)(i.br,{}),"\n","Low Clip Black Scene Referred Linear value below 0.0001762728758.",(0,n.jsx)(i.br,{}),"\n","-10 EV Purple Scene Referred Linear value 0.0001762728758.",(0,n.jsx)(i.br,{}),"\n","-7 EV Blue Scene Referred Linear value 0.001404109349.",(0,n.jsx)(i.br,{}),"\n","-4 EV Cyan Scene Linear value 0.01124714399.",(0,n.jsx)(i.br,{}),"\n","-2 EV Green Scene Referred Linear value 0.04456791864.",(0,n.jsx)(i.br,{}),"\n","0 EV Grey Scene Referred Linear value 0.18009142.",(0,n.jsx)(i.br,{}),"\n","+2 EV Green Scene Referred Linear value 0.7196344767.",(0,n.jsx)(i.br,{}),"\n","+4 EV Yellow Scene Referred Linear value 2.883658483.",(0,n.jsx)(i.br,{}),"\n","+5.5 EV Red Scene Referred Linear value 8.150007644.",(0,n.jsx)(i.br,{}),"\n","High Clip White Scene Referred Linear value above 16.29174024."]}),"\n",(0,n.jsx)(i.h2,{id:"diagnostics",children:"Diagnostics"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"False Color to determine the the blown out."}),"\n",(0,n.jsx)(i.li,{children:'If the first figure is much larger than the second, this means that the system is doing a lot of out-of-core memory transfers and performance will be affected. For example, if it said "100GB / 4GB", this would mean that Redshift had reserved 4GB for the geometry (triangles, hairs, particles) but because the scene was complicated, 100GB of data had to go back and forth between the CPU and GPU memory.'}),"\n",(0,n.jsx)(i.li,{}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"interiors",children:"Interiors"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"USE IRRADIANCE CACHE ON INTERIORS. SAVE A RIDICULOUSLY A LOT OF TIME."}),"\n",(0,n.jsx)(i.li,{children:"When a domelight is present in the scene in replaces any rsEnvironment that you have. The portal lights will sample the rsEnvironment directly behind them but since you are not using the rsEnvironment you are not sampling the domelight efficiently. Either disable the dome light environment or light the scene only with the dome light. The portal lights use Multi Importance Sampling through the rsEnv so they are effective for interiors."}),"\n",(0,n.jsx)(i.li,{children:"A 16/64/0.003 unified, 1024 samples for the lights and 1024 samples for GI should do the trick,"}),"\n",(0,n.jsx)(i.li,{children:"IC is very good for flat surfaces but not so god on vegetation so set your plants to forced BF."}),"\n",(0,n.jsx)(i.li,{children:"The prepass render only computes GI nothing else, once that has been computed you simply set the GI method to load instead of rebuild. Then the final render will skip the already computed GI. No loss in render times."}),"\n",(0,n.jsx)(i.li,{children:"You can also try to try the GI compute with a single frame by enabling motion blur and set the length to the length of the animation, this will compute the GI for one frame but it will be the same for the whole animation, then just load it"}),"\n",(0,n.jsx)(i.li,{children:"It's fine to have the IC or IPC preview showing a lot of fireflies, don't worry about it."}),"\n",(0,n.jsx)(i.li,{children:"You might be better of with just a dome light instead of portals if the hdr light is too contrasty. Portals are perfect with diffuse hdrs not great with tiny light source data inside the hdr texture due to missing MIS"}),"\n",(0,n.jsx)(i.li,{children:"Generally speaking you don't use portals when using a dome light. You use portals when working with the environment light. Usually works best for interiors while domelights work better for wide open lighting situations. But as Adrian mentioned the domelight does do a bit better when there is a very intense high contrast lighting HDR. So it will depend on your scene and HDR which method works best."}),"\n",(0,n.jsx)(i.li,{children:"Reduce your direct light cut-offs."}),"\n",(0,n.jsx)(i.li,{children:"Well, through testing, it seems that oddly enough, placing a plane object with a glass material right outside the window and then setting the fresnel type to metalness on that material brings down the blown out whiteness quite a bit."}),"\n",(0,n.jsx)(i.li,{children:"Then it is normal. The stronger the bokeh, the more samples you will need. In scenes with Motion blur you can get away with lower samples, but in architectural interiors, noise is easy to spot on. Give a try to Altus denoising."}),"\n",(0,n.jsx)(i.li,{children:"Controlling final the image in post is important for interiors. Tweaking your curves in particular can really sell the realism. A cryptomatte pass provides even more control."}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"My suggestion for interior:\nBF+IPC\nBF rays 1024 or 2048\nIPC SPP 16 or 32, retrace 2 or 4. Screen radius 4 will make it go noticeably faster, but can fail if you have thin geometry in my experience. IPC settings are mostly only needed to be changed if there is object + camera animation, and maybe light animation as well, though."}),"\n",(0,n.jsx)(i.p,{children:"By the way, I would bet that most of the rendering time is because of the semi-transparent curtains. These tend to kick our ass when it comes to irradiance caching and this is something we've been meaning to look at for some time. When things clear up a bit, we'll revisit this. In the meantime, if you have 'thickness' in these curtains, I would suggest you make them single sided 'sheets' of geometry."}),"\n",(0,n.jsx)(i.p,{children:"I would usually have the small overhead lights as incandescent materials with GI set to off, or actual lights set to only affect the fixture they're in. Then use large area lights to produce the actual light."}),"\n",(0,n.jsxs)(i.p,{children:["Env is the rsSky so they pick up the sky env to cast light inside. If you have glass in your windows ",(0,n.jsx)(i.strong,{children:"turn off their shadow casting"})," there's no need for that."]}),"\n",(0,n.jsx)(i.p,{children:"Please remember that in the real-world all lights have a size while in CG they can be abstract and have no size (sun, point, spot). So always use a light with a size. The intensity of the BRDF is controlled by the reflections section of your material. Please have a quick read on how the rsMaterial behaves and its controls. here:"}),"\n",(0,n.jsx)(i.p,{children:"Just wanted to remind you that a mesh light needs GI to be turned on to clean up the noise so if there's no GI in the scene you can increase the samples to whatever value it won't clean it up properly.\nA mesh light \"samples\" noise on both direct and indirect rays that's why it needs GI to be on. So those 36 seconds if GI off, might turn into 10 if GI is on, but if GI was on then maybe the scene requires that many samples.\nHope it helps."}),"\n",(0,n.jsx)(i.p,{children:"You might be better off with just setting these detailed objects to force brute force in their object properties, and let the large surfaces likes walls and floors stay IC"}),"\n",(0,n.jsx)(i.p,{children:"There are also ways of making IPC faster, at the loss of some detail. Lowering its radius to 4 will speed things up."}),"\n",(0,n.jsx)(i.p,{children:"I suggest you force BF on those cornices since for small details IC might be even slower than BF."}),"\n",(0,n.jsx)(i.p,{children:'Also I would rather have 8-64 unified samples, but raise the error threshold to maybe 0.02 if it still looks ok. Would also turn on "randomize pattern on each frame" so that the noise won\'t be stuck to your screen.'}),"\n",(0,n.jsx)(i.p,{children:'Lowering "max subsample intensity" and "max secondary ray intensity" to 1.2 can also save a little bit, but this will remove some of the range in the image, and make it feel a bit flatter.'}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsxs)(i.p,{children:["If the performance is low ",(0,n.jsx)(i.strong,{children:"because the scene has many lights,"})," I'd start by increasing the direct lighting cutoff."]}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"PORTALS MANY or JUST ONE LARGE"}),"\n",(0,n.jsx)(i.p,{children:"The other suggestions offered here by other people are good. If you tried mesh lights and got noise, you should raise the mesh light's num samples."}),"\n",(0,n.jsx)(i.p,{children:"Yes, it's likely that the BF rays need to be raised, defaults of 16 I think, are just for test renders and useless for final quality."}),"\n",(0,n.jsx)(i.p,{children:"Please note that IC is not very efficient when dealing with a lot of tiny objects that is why BF is a way better solution for these scenarios.\nNoise wise, turn off GI and if it's clean then simply increase the GI samples until you're satisfied."}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"Are you using an RS dome light (hdr) in your image? Try setting the correct color profile first on that image (and not use auto) try setting it to 'Scene-linear Rec.709-sRGB' and see if that helps with getting less saturated accurate colors."}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"Linear Rec 709 sRGB, sRGB, Tone Un-Mapped"}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"No, that is correct, the portal lights don't have Image based Multi Importance Sampling so they need a lot of samples to cleanup compared to a dome light which has MIS. The same happens with a textured Area Light compared to a non textured Area Light. Thus the difference in render times, diffuse environments are fast, contrasty environments are slow."}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"I see... you will not use portals in general when in comes to HDRI-Lighting. Am I right? You will use it in conjunction with Redshift Sky and RS Sun. Right?"}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"Is the env using a high contrast HDRI or a low contrast one? If it's high contrast a dome light without portal is more efficient, if it's low contrast then the portals should do a better job"}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"Portal lights have no MIS (Multi Importance Sampling) support so it's harder for them to clean up noise than a regular dome light which has MIS support. Portals are effective when the env associated with them is fairly low contrast and diffuse."}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"To be efficient a portal light requires a diffuse env not a sharp env as it cannot do Multi Importance Sampling. When the env texture is sharp you're better off with a dome light instead because it cannot do the MIS on the texture directly as it does it for dome lights. You will notice this on textured lights or portals (which are in essence textured lights)"}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"If you want to keep that specific texture either switch to a dome light or use a high number of samples for the lights or use auto sampling"}),"\n"]}),"\n",(0,n.jsxs)(i.blockquote,{children:["\n",(0,n.jsx)(i.p,{children:"Is it pointing the right way? Also, be sure to place it outside the window, not in it."}),"\n"]})]})}function c(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,i,t)=>{t.d(i,{R:()=>o,x:()=>l});var n=t(6540);const s={},r=n.createContext(s);function o(e){const i=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),n.createElement(r.Provider,{value:i},e.children)}}}]);